---
title: "Unit 4 - Parallel and Fast"
engine: julia
julia:
    exeflags: ["--project=@.", "--threads=8"]
    author: 
        - "Aapeli Vuorinen"
        - "Yoni Nazarathy"
---

For decades, computers got faster mainly because chipmakers squeezed more transistors onto chips and raised their clock speeds—this was Moore’s law. But after the early 2000s, increasing clock speeds hit limits due to heat and power. Instead of making a single core faster, CPUs started coming with multiple cores so computers could work on several things at once. GPUs, which were designed for graphics, also became much more powerful and began to be used for general-purpose computing. Now, to speed up programs, we have to use parallel computing across multicore CPUs, GPUs, and even multiple machines working together—this is called distributed computing. See for example the article @sutter2005free.

Julia is well suited for distributed computing, parallel computing, and GPU use. See the overview of [parallel computing](https://docs.julialang.org/en/v1/manual/parallel-computing/) in the Julia documentation. In this unit we deal with several layers of parallelism for speedup: 

1. **Multi-threading** - Use multiple CPU threads to run tasks simultaneously within a single process. See the [docs](https://docs.julialang.org/en/v1/manual/multi-threading/). 
1. **Distributed Computing** - Use multiple processes, possibly on different machines, to work together on larger problems. See the [docs](https://docs.julialang.org/en/v1/manual/distributed-computing/) as well as [Distributed.jl](https://github.com/JuliaLang/Distributed.jl) and [Malt.jl](https://github.com/JuliaPluto/Malt.jl).
1. **GPUs** - Harness the massive parallelism of graphics processors for compute-heavy tasks. See [JuliaGPU.org](https://juliagpu.org/).

Combinations of 1, 2, and 3 are obviously possible. 

Note that a related item, also in the Julia documentation is "Asynchronous "tasks", or coroutines". See the [Asynchronous Programming docs](https://docs.julialang.org/en/v1/manual/asynchronous-programming/). 

We now focus on multi-threading, distributed computing, and GPUS. We then attempt to run and example with [Oceananigans.jl](https://github.com/CliMA/Oceananigans.jl) from the [CliMA organization](https://juliapackages.com/u/clima). See the recent preprint @wagner2025high.

# Multi-threading

This quarto notebook was generated with the flag `-t 8` (or `--threads 8`). It means there are 8 threads available. Without it there is only 1 thread. If you try in the command line `julia --help` you'll see:

```
 -t, --threads {auto|N[,auto|M]}
                           Enable N[+M] threads; N threads are assigned to the `default`
                           threadpool, and if M is specified, M threads are assigned to the
                           `interactive` threadpool; `auto` tries to infer a useful
                           default number of threads to use but the exact behavior might change
                           in the future. Currently sets N to the number of CPUs assigned to
                           this Julia process based on the OS-specific affinity assignment
                           interface if supported (Linux and Windows) or to the number of CPU
                           threads if not supported (MacOS) or if process affinity is not
                           configured, and sets M to 1.
```

Let's use the `nthreads` function in runtime to see how many threads there are:

```{julia}
Threads.nthreads()
```

As you look at the output, to get a grasp of when things are executed let's define the `time_stamp` function which returns a time stamp in a resolution of a tenth of a second. Remember that there are `86400` seconds in a day, so expect the values of this function to reach almost up to $10^6$, to be used as a relative timestamp:

```{julia}
using Dates

"""
Returns how many tenths of seconds passed since midnight
"""
function time_stamp()
    now = Dates.now()
    midnight = DateTime(Date(now))  # Today at 00:00:00
    return Millisecond(now - midnight).value ÷ 100
end;
```

For basic parallelism, one of the best tools we have is the [`Threads.@threads`](https://docs.julialang.org/en/v1/base/multi-threading/#Base.Threads.@threads) macro. Now since we have 8 threads in our system, observe that this loop essentially runs in three batches.

```{julia}
before_start = time_stamp()
Threads.@threads for i in 1:17
    println(time_stamp(), 
            " "^(time_stamp()-before_start), # spacing
            ": Starting iteration $i")

    sleep(1) # sleep for one second as though "processing something"

    println(time_stamp(),
            " "^(time_stamp()-before_start), # spacing
             ": Finished sleeping (\"processing\") on thread $(Threads.threadid())")
end
```

## Example - Simple Number Theory Computation

The [Goldbach Conjecture](https://en.wikipedia.org/wiki/Goldbach%27s_conjecture):

```{julia}
using BenchmarkTools
using Plots

function sieve_of_Eratosthenes(n)
    primebits = ones(Bool,n) # Will contain true if the index is prime (initially all assumed prime)
    primebits[1] = false # The number 1 is not prime
    p = 2 # Smallest prime
    @inbounds while p ≤ n
        i = 2p
        while i ≤ n  # \le +[TAB]
            primebits[i] = false
            i += p
        end
        p += 1
        while p ≤ n && !primebits[p]
            p += 1
        end
    end
    return primebits
end

function check_Goldbachs_slow(n)
    primebits = sieve_of_Eratosthenes(n)
    primes = (1:n)[primebits]
    out = zeros(Int, n)

    for i in 2:2:n
        for p in primes
            if in(i - p, primes)
                out[i] += 1
            end
        end
    end
    return out[2:2:n]
end

function check_Goldbachs(n::Integer)
    # Below we are working in the subset of the numbers
    # The primes we care about are odd numbers in the range 3:2:n
    # The output are even numbers in the range 2:2:n

    n_half = n >> 0x01
    primebits = sieve_of_Eratosthenes(n)
    @inbounds mask = primebits[3:2:n]
    @inbounds primes = (Int32(1):Int32(n_half - 1))[mask] # not really the primes!

    out = zeros(UInt16, n_half)
    out[2] = 0x0001 # 4 is 2 + 2, and below we only deal with odd primes
    @inbounds for x in primes
        i_out = x + 2
        for i_mask in 1:(n_half - x)
            out[i_out] += mask[i_mask]
            i_out += 1
        end
    end
    return out
end

# Test the implementation
@assert check_Goldbachs_slow(100) == check_Goldbachs(100)

n = 10^4#1_000_000
checks = @btime check_Goldbachs(n)

scatter(2:2:n, checks, legend=false, xlabel="n", ylabel="Number of Goldbach pairs", markersize=0.1)
```


# Distributed computing

* [Distributed.jl](https://github.com/JuliaLang/Distributed.jl)
* [Malt.jl](https://github.com/JuliaPluto/Malt.jl)

# GPUs

* [juliagpu.org](https://juliagpu.org/)
* [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl)

# Example - Oceananigans 

Let's explore the [Oceananigans.jl](https://github.com/CliMA/Oceananigans.jl) package and try to run it both on GPU and on CPU.

# Additional online resources

* An incredible [Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications](https://book.sciml.ai/) course/book by [Chris Rackauckas](https://chrisrackauckas.com/).
* A [Julia for high-performance scientific computing](https://enccs.github.io/julia-for-hpc/) course.
* Another [High Performance Course](https://github.com/carstenbauer/JuliaHLRS22?tab=readme-ov-file).

# Exercises

1. Carry out the "Multithread the computation of π" exercise [provided here](https://enccs.github.io/julia-for-hpc/multithreading/).
1. Carry out the "Distribute the computation of π" exercise [provided here](https://enccs.github.io/julia-for-hpc/distributed/).
1. Carry out the "Port `sqrt_sum()` to GPU" exercise [provided here](https://enccs.github.io/julia-for-hpc/GPU/)

