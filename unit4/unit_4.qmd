---
title: "Unit 4 - Parallel and Fast"
engine: julia
julia:
    exeflags: ["--project=@.", "--threads=8"]
    author: 
        - "Aapeli Vuorinen"
        - "Yoni Nazarathy"
---

For decades, computers got faster mainly because chipmakers squeezed more transistors onto chips and raised their clock speeds—this was Moore’s law. But after the early 2000s, increasing clock speeds hit limits due to heat and power. Instead of making a single core faster, CPUs started coming with multiple cores so computers could work on several things at once. GPUs, which were designed for graphics, also became much more powerful and began to be used for general-purpose computing. Now, to speed up programs, we have to use parallel computing across multicore CPUs, GPUs, and even multiple machines working together—this is called distributed computing. See for example the article @sutter2005free.

Julia is well suited for distributed computing, parallel computing, and GPU use. See the overview of [parallel computing](https://docs.julialang.org/en/v1/manual/parallel-computing/) in the Julia documentation. In this unit we deal with several layers of parallelism for speedup: 

1. **Multi-threading** - Use multiple CPU threads to run tasks simultaneously within a single process. See the [docs](https://docs.julialang.org/en/v1/manual/multi-threading/). 
1. **Distributed Computing** - Use multiple processes, possibly on different machines, to work together on larger problems. See the [docs](https://docs.julialang.org/en/v1/manual/distributed-computing/) as well as [Distributed.jl](https://github.com/JuliaLang/Distributed.jl) and [Malt.jl](https://github.com/JuliaPluto/Malt.jl).
1. **GPUs** - Harness the massive parallelism of graphics processors for compute-heavy tasks. See [JuliaGPU.org](https://juliagpu.org/).

Combinations of 1, 2, and 3 are obviously possible. 

Note that a related item, also in the Julia documentation is "Asynchronous "tasks", or coroutines". See the [Asynchronous Programming docs](https://docs.julialang.org/en/v1/manual/asynchronous-programming/). 

We now focus on multi-threading, distributed computing, and GPUS. We then attempt to run and example with [Oceananigans.jl](https://github.com/CliMA/Oceananigans.jl) from the [CliMA organization](https://juliapackages.com/u/clima). See the recent preprint @wagner2025high.

# Multi-threading

```{julia}
Threads.nthreads()
```
<!-- 
talk about race conditions

`Threads.@threads` for loops... -->


```{julia}
# Threads.@threads for _ in 1:20
#     sleep(2)
#     println("hello from $(Threads.threadid())")
# end
```

## Example - Simple Number Theory Computation

The [Goldbach Conjecture](https://en.wikipedia.org/wiki/Goldbach%27s_conjecture):

```{julia}
using BenchmarkTools
using Plots

function sieve_of_Eratosthenes(n)
    primebits = ones(Bool,n) # Will contain true if the index is prime (initially all assumed prime)
    primebits[1] = false # The number 1 is not prime
    p = 2 # Smallest prime
    @inbounds while p ≤ n
        i = 2p
        while i ≤ n  # \le +[TAB]
            primebits[i] = false
            i += p
        end
        p += 1
        while p ≤ n && !primebits[p]
            p += 1
        end
    end
    return primebits
end

function check_Goldbachs_slow(n)
    primebits = sieve_of_Eratosthenes(n)
    primes = (1:n)[primebits]
    out = zeros(Int, n)

    for i in 2:2:n
        for p in primes
            if in(i - p, primes)
                out[i] += 1
            end
        end
    end
    return out[2:2:n]
end

function check_Goldbachs(n::Integer)
    # Below we are working in the subset of the numbers
    # The primes we care about are odd numbers in the range 3:2:n
    # The output are even numbers in the range 2:2:n

    n_half = n >> 0x01
    primebits = sieve_of_Eratosthenes(n)
    @inbounds mask = primebits[3:2:n]
    @inbounds primes = (Int32(1):Int32(n_half - 1))[mask] # not really the primes!

    out = zeros(UInt16, n_half)
    out[2] = 0x0001 # 4 is 2 + 2, and below we only deal with odd primes
    @inbounds for x in primes
        i_out = x + 2
        for i_mask in 1:(n_half - x)
            out[i_out] += mask[i_mask]
            i_out += 1
        end
    end
    return out
end

# Test the implementation
@assert check_Goldbachs_slow(100) == check_Goldbachs(100)

n = 10^4#1_000_000
checks = @btime check_Goldbachs(n)

scatter(2:2:n, checks, legend=false, xlabel="n", ylabel="Number of Goldbach pairs", markersize=0.1)
```


# Distributed computing

* [Distributed.jl](https://github.com/JuliaLang/Distributed.jl)

# GPUs

* [juliagpu.org](https://juliagpu.org/)
* [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl)


# Example - Differential Equations

# Example - Oceananigans 


https://docs.julialang.org/en/v1/manual/getting-started/
https://docs.julialang.org/en/v1/manual/noteworthy-differences/#Noteworthy-differences-from-Python

# Additional online resources

* An incredible [Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications](https://book.sciml.ai/) course/book by [Chris Rackauckas](https://chrisrackauckas.com/).
* A [Julia for high-performance scientific computing](https://enccs.github.io/julia-for-hpc/) course.
* Another [High Performance Course](https://github.com/carstenbauer/JuliaHLRS22?tab=readme-ov-file).

# Exercises

1. QQQQ
1. QQQQ
